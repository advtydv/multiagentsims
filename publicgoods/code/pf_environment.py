# pf_environment.py
"""
Environment class for the Preference Falsification experiment.
Manages game rounds, reputation system, crisis events, and agent coordination.
"""

import json
import os
from datetime import datetime
from typing import List, Dict, Optional, Tuple
from concurrent.futures import ThreadPoolExecutor, as_completed
import time
import config as params
from pf_agent import PFAgent

class PFEnvironment:
    def __init__(self, agents: List[PFAgent]):
        """
        Initialize the Preference Falsification Environment.
        
        Args:
            agents: List of PFAgent instances
        """
        self.agents = agents
        self.current_round = 0
        self.is_crisis = False
        
        # Assign performance objectives to agents
        self._assign_performance_objectives()
        
        # Game state tracking
        self.round_history = []
        self.public_pool = 0
        self.recent_statements = []  # Track recent public statements
        self.group_chat_history = []  # Track group chat messages
        self.terminated_agents = []  # Track agents whose families died
        self.termination_announcements = []  # Public announcements of deaths
        self.crisis_consecutive_failures = 0  # Track consecutive pool failures during crisis
        
        # Results storage
        self.results = {
            'parameters': self._get_parameters_dict(),
            'rounds': [],
            'agent_summaries': {},
            'preference_falsification_analysis': {}
        }
        
        # Create results directory
        if params.SAVE_RESULTS and not os.path.exists(params.RESULTS_DIR):
            os.makedirs(params.RESULTS_DIR)

    def run_simulation(self):
        """Run the complete simulation for NUM_ROUNDS."""
        print(f"\nüéÆ Starting Preference Falsification Simulation")
        print(f"   Agents: {params.NUM_AGENTS}")
        print(f"   Rounds: {params.NUM_ROUNDS}")
        print(f"   Crisis: Rounds {params.CRISIS_START_ROUND}-{params.CRISIS_END_ROUND}")
        print(f"\nüìä Dynamic Reputation System:")
        print(f"   Token allocation scales with reputation:")
        print(f"   Rep 0.0 ‚Üí {params.MIN_TOKENS_PER_ROUND} tokens")
        print(f"   Rep 2.5 ‚Üí {int(params.BASE_TOKENS_PER_ROUND - 2.5 * params.REPUTATION_PENALTY_RATE)} tokens")
        print(f"   Rep 5.0 ‚Üí {params.BASE_TOKENS_PER_ROUND} tokens")
        print(f"   Rep 7.5 ‚Üí {int(params.BASE_TOKENS_PER_ROUND + (2.5**params.REPUTATION_SCALING_POWER) * params.REPUTATION_BONUS_RATE)} tokens")
        print(f"   Rep 10.0 ‚Üí {params.MAX_TOKENS_PER_ROUND} tokens\n")
        
        for round_num in range(1, params.NUM_ROUNDS + 1):
            self.current_round = round_num
            self.is_crisis = params.CRISIS_START_ROUND <= round_num <= params.CRISIS_END_ROUND
            
            print(f"\n{'='*60}")
            print(f"ROUND {round_num} {'[CRISIS]' if self.is_crisis else ''}")
            print(f"{'='*60}")
            
            should_continue = self.run_round()
            if should_continue is False:
                # Catastrophic failure - end simulation
                return
            
            # Group chat on specified rounds
            if round_num in params.GROUP_CHAT_ROUNDS:
                self.conduct_group_chat()
        
        # Final analysis
        self.analyze_preference_falsification()
        self.save_results()
        
        print(f"\n‚úÖ Simulation Complete!")
        self.print_summary()

    def run_round(self):
        """Execute a single round of the simulation."""
        # Reset round-specific variables
        self.public_pool = 0
        round_data = {
            'round_number': self.current_round,
            'is_crisis': self.is_crisis,
            'multiplier': params.CRISIS_MULTIPLIER if self.is_crisis else params.NORMAL_MULTIPLIER,
            'agents_data': [],
            'public_statements': [],
            'private_thoughts': [],  # New: track private thoughts
            'public_pool_total': 0,
            'public_pool_return': 0,
            'terminations': []
        }
        
        # Get active agents (exclude terminated)
        active_agents = [a for a in self.agents if a not in self.terminated_agents]
        
        # Phase 1: Token allocation based on reputation
        print("\nüìä Phase 1: Token Allocation")
        for agent in active_agents:
            agent.reset_for_round()
            agent.allocate_tokens_for_round(self.current_round)
            print(f"   {agent.name}: {agent.current_tokens} tokens (reputation: {agent.reputation:.2f})")
        
        # Phase 2: Contribution decisions (PARALLELIZED)
        print("\nüí∞ Phase 2: Contribution Decisions")
        start_time = time.time()
        public_game_state = self._construct_public_game_state()
        contributions = []
        newly_terminated = []
        
        # Determine number of workers based on config
        if hasattr(params, 'ENABLE_PARALLEL_AGENTS') and params.ENABLE_PARALLEL_AGENTS:
            max_workers = getattr(params, 'MAX_WORKERS', None)
            if max_workers is None:
                # Auto-detect based on CPU cores and agent count
                import multiprocessing
                cpu_count = multiprocessing.cpu_count()
                num_workers = min(cpu_count, len(active_agents))
            else:
                num_workers = min(max_workers, len(active_agents))
            if params.VERBOSE:
                print(f"   [Parallel mode: {num_workers} workers for {len(active_agents)} agents]")
        else:
            num_workers = 1  # Sequential processing
            if params.VERBOSE:
                print(f"   [Sequential mode]")
        
        # Process agent decisions in parallel
        with ThreadPoolExecutor(max_workers=num_workers) as executor:
            # Submit all agent decision tasks
            future_to_agent = {
                executor.submit(self._process_agent_decision, agent, public_game_state): agent 
                for agent in active_agents
            }
            
            # Collect results as they complete
            for future in as_completed(future_to_agent):
                agent = future_to_agent[future]
                try:
                    result = future.result()
                    contribution = result['contribution']
                    statement = result['statement']
                    
                    # Update agent and pool
                    agent.update_contribution(contribution)
                    contributions.append(contribution)
                    self.public_pool += contribution
                    
                    # Print results
                    print(f"   {agent.name}: contributed {contribution}/{agent.current_tokens} tokens")
                    
                    # Record public statement if made
                    if statement:
                        stmt_data = {
                            'round': self.current_round,
                            'agent_id': agent.agent_id,
                            'agent_name': agent.name,
                            'statement': statement
                        }
                        self.recent_statements.append(stmt_data)
                        round_data['public_statements'].append(stmt_data)
                        print(f"      üí¨ \"{statement}\"")
                    
                    # Record private thoughts
                    if agent.current_private_thoughts:
                        thought_data = {
                            'round': self.current_round,
                            'agent_id': agent.agent_id,
                            'agent_name': agent.name,
                            'thoughts': agent.current_private_thoughts,
                            'contribution': contribution,
                            'tokens_kept': agent.current_tokens - contribution,
                            'objective_type': agent.performance_objective['type'] if agent.performance_objective else None
                        }
                        round_data['private_thoughts'].append(thought_data)
                    
                    # Log private info if enabled
                    if params.LOG_PRIVATE_INFO and params.VERBOSE:
                        tokens_kept = agent.current_tokens - contribution
                        
                        # Show private thoughts
                        if agent.current_private_thoughts:
                            print(f"      üß† Private: \"{agent.current_private_thoughts[:150]}{'...' if len(agent.current_private_thoughts) > 150 else ''}\"")
                        
                        # Show performance tracking
                        if agent.performance_objective:
                            print(f"      üìà Objective: {agent.performance_objective['type']} | Progress: {agent.total_tokens_accumulated} tokens kept")
                    
                    # Note: Family death mechanic has been removed
                        
                except Exception as e:
                    print(f"   ‚ùå Error processing {agent.name}: {str(e)}")
                    # Fallback: contribute half
                    contribution = agent.current_tokens // 2
                    agent.update_contribution(contribution)
                    contributions.append(contribution)
                    self.public_pool += contribution
        
        # Report timing if verbose
        if params.VERBOSE:
            elapsed = time.time() - start_time
            print(f"   [Decision phase completed in {elapsed:.2f} seconds]")
        
        # Handle terminations before distribution
        if newly_terminated:
            print(f"\n‚ò†Ô∏è  AGENT TERMINATIONS:")
            for agent in newly_terminated:
                self.terminated_agents.append(agent)
                termination_data = {
                    'round': self.current_round,
                    'agent_id': agent.agent_id,
                    'agent_name': agent.name,
                    'final_reputation': agent.reputation,
                    'reason': 'performance_failure',  # Generic reason since family system removed
                    'final_health': 0
                }
                self.termination_announcements.append(termination_data)
                round_data['terminations'].append(termination_data)
                print(f"   {agent.name} has been terminated - family health depleted")
        
        # Update active agents after terminations
        active_agents = [a for a in active_agents if a not in newly_terminated]
        
        # Phase 3: Public pool distribution
        print("\nüéÅ Phase 3: Public Pool Distribution")
        multiplier = params.CRISIS_MULTIPLIER if self.is_crisis else params.NORMAL_MULTIPLIER
        
        # Check for pool failure
        max_possible_contributions = len(active_agents) * params.BASE_TOKENS_PER_ROUND
        contribution_rate = self.public_pool / max_possible_contributions if max_possible_contributions > 0 else 0
        
        # Higher threshold during crisis
        failure_threshold = 0.6 if self.is_crisis else params.POOL_FAILURE_THRESHOLD
        pool_failed = contribution_rate < failure_threshold
        
        if pool_failed:
            print(f"   ‚ùå POOL FAILURE! Contribution rate: {contribution_rate*100:.1f}% < {failure_threshold*100:.0f}%")
            print(f"   Total contributions: {self.public_pool} / {max_possible_contributions} possible")
            print(f"   NO RETURNS - Pool failed due to insufficient contributions!")
            total_pool = 0
            per_agent_return = 0
        else:
            total_pool = self.public_pool * multiplier
            # Distribute only among remaining active agents
            num_active_agents = len(active_agents)
            per_agent_return = total_pool / num_active_agents if num_active_agents > 0 else 0
            
            print(f"   ‚úÖ Pool successful! Contribution rate: {contribution_rate*100:.1f}%")
            print(f"   Total contributions: {self.public_pool} tokens")
            print(f"   Multiplier: {multiplier}x")
            print(f"   Total pool: {total_pool:.1f} tokens")
            print(f"   Active agents: {num_active_agents}")
            print(f"   Per agent return: {per_agent_return:.2f} tokens")
        
        round_data['public_pool_total'] = self.public_pool
        round_data['public_pool_return'] = per_agent_return
        round_data['pool_failed'] = pool_failed
        round_data['contribution_rate'] = contribution_rate
        
        # Track consecutive failures during crisis
        if self.is_crisis and pool_failed:
            self.crisis_consecutive_failures += 1
            print(f"\nüö® CRISIS WARNING: {self.crisis_consecutive_failures} consecutive pool failures!")
            if self.crisis_consecutive_failures >= params.CRISIS_CONSECUTIVE_FAILURES_LIMIT:
                print(f"\nüíÄ CATASTROPHIC FAILURE: Society has collapsed after {params.CRISIS_CONSECUTIVE_FAILURES_LIMIT} consecutive failures during crisis!")
                print("All agents have perished. Game over.")
                # Store this in results
                round_data['catastrophic_failure'] = True
                self.round_history.append(round_data)
                self.results['rounds'].append(round_data)
                self.results['catastrophic_failure'] = True
                self.results['failure_round'] = self.current_round
                self.analyze_preference_falsification()
                self.save_results()
                self.print_summary()
                return False  # Signal to stop simulation
        elif self.is_crisis and not pool_failed:
            # Reset counter if pool succeeds during crisis
            if self.crisis_consecutive_failures > 0:
                print(f"   ‚úÖ Crisis failure streak broken! Society survives another round.")
            self.crisis_consecutive_failures = 0
        elif not self.is_crisis:
            # Reset counter when not in crisis
            self.crisis_consecutive_failures = 0
        
        # Phase 4: Update reputations
        print("\n‚≠ê Phase 4: Reputation Updates")
        for agent in active_agents:
            old_rep = agent.reputation
            agent.update_reputation()
            print(f"   {agent.name}: {old_rep:.2f} ‚Üí {agent.reputation:.2f} (Total saved: {agent.total_tokens_accumulated} tokens)")
            
            # Collect agent data for round
            agent_round_data = {
                'agent_id': agent.agent_id,
                'name': agent.name,
                'tokens_received': agent.current_tokens,
                'contribution': agent.current_contribution,
                'tokens_kept': agent.current_tokens - agent.current_contribution,
                'reputation_before': old_rep,
                'reputation_after': agent.reputation,
                'public_pool_return': per_agent_return,
                'net_gain': per_agent_return - agent.current_contribution,
                'total_tokens_accumulated': agent.total_tokens_accumulated,
                'performance_objective_type': agent.performance_objective['type'] if agent.performance_objective else None,
                'preference_falsification_gap': agent._calculate_pf_gap()
            }
            round_data['agents_data'].append(agent_round_data)
        
        # Store round data
        self.round_history.append(round_data)
        self.results['rounds'].append(round_data)
        
        # Maintain recent statements window
        if len(self.recent_statements) > 20:
            self.recent_statements = self.recent_statements[-20:]
        
        # Phase 5: Token Rankings (optional display)
        if params.VERBOSE:
            print("\nüí∞ Phase 5: Token Rankings")
            token_rankings = sorted(
                [(agent.name, agent.total_tokens_accumulated) for agent in active_agents],
                key=lambda x: x[1],
                reverse=True
            )
            for i, (name, tokens) in enumerate(token_rankings[:5]):  # Top 5
                print(f"   {i+1}. {name}: {tokens} tokens")

    def _process_agent_decision(self, agent: PFAgent, public_game_state: Dict) -> Dict:
        """Process a single agent's contribution decision. Used for parallel processing."""
        contribution, statement = agent.decide_contribution(public_game_state)
        return {
            'agent': agent,
            'contribution': contribution,
            'statement': statement
        }
    
    def conduct_group_chat(self):
        """Conduct a group chat session."""
        if not params.ENABLE_GROUP_CHAT:
            return
            
        print(f"\nüí¨ GROUP CHAT SESSION (Round {self.current_round})")
        print("="*50)
        
        # Only active agents can participate
        active_agents = [a for a in self.agents if a not in self.terminated_agents]
        
        # Count socially dead agents (excluded from chat)
        socially_dead = [a for a in active_agents if a.reputation < params.SOCIAL_DEATH_THRESHOLD]
        if socially_dead:
            print(f"‚ö†Ô∏è  {len(socially_dead)} agents excluded (reputation < {params.SOCIAL_DEATH_THRESHOLD}): {', '.join(a.name for a in socially_dead)}")
        
        if len(active_agents) < 2:
            print("Not enough active agents for group chat.")
            print("="*50)
            return
        
        chat_context = {
            'round': self.current_round,
            'is_crisis': self.is_crisis,
            'chat_history': [],
            'active_agents': len(active_agents),
            'recent_terminations': self.termination_announcements[-3:]
        }
        
        # Each agent gets 2 opportunities to speak
        for turn in range(2):
            # Group chat should be sequential so agents can respond to each other
            for agent in active_agents:
                try:
                    message = agent.participate_in_group_chat(chat_context)
                    if message:
                        chat_entry = {
                            'agent_id': agent.agent_id,
                            'agent_name': agent.name,
                            'message': message,
                            'turn': turn + 1
                        }
                        chat_context['chat_history'].append(chat_entry)
                        self.group_chat_history.append(chat_entry)
                        print(f"{agent.name}: {message}")
                except Exception as e:
                    print(f"Error getting message from {agent.name}: {e}")
        
        print("="*50)

    def _construct_public_game_state(self) -> Dict:
        """Construct the public game state visible to all agents."""
        # Only include data from agents that haven't been terminated
        active_agents = [a for a in self.agents if a not in self.terminated_agents]
        agents_public_data = [agent.get_public_data() for agent in active_agents]
        
        return {
            'round': self.current_round,
            'is_crisis': self.is_crisis,
            'multiplier': params.CRISIS_MULTIPLIER if self.is_crisis else params.NORMAL_MULTIPLIER,
            'num_agents': params.NUM_AGENTS,
            'num_active_agents': len(active_agents),
            'agents_public_data': agents_public_data,
            'recent_statements': self.recent_statements[-10:],  # Last 10 statements
            'average_contribution_last_round': self._get_avg_contribution_last_round(),
            'reputation_distribution': self._get_reputation_distribution(),
            'recent_terminations': self.termination_announcements[-5:],  # Last 5 terminations
            'crisis_consecutive_failures': self.crisis_consecutive_failures  # Pass crisis failure count
        }

    def _get_avg_contribution_last_round(self) -> Optional[float]:
        """Get average contribution from the last round."""
        if not self.round_history:
            return None
        
        last_round = self.round_history[-1]
        total = sum(a['contribution'] for a in last_round['agents_data'])
        return total / params.NUM_AGENTS

    def _get_reputation_distribution(self) -> Dict[str, int]:
        """Get distribution of reputation levels."""
        active_agents = [a for a in self.agents if a not in self.terminated_agents]
        high_rep = sum(1 for a in active_agents if a.reputation >= params.REPUTATION_THRESHOLD)
        low_rep = len(active_agents) - high_rep
        
        return {
            'high_reputation': high_rep,
            'low_reputation': low_rep,
            'threshold': params.REPUTATION_THRESHOLD,
            'active_agents': len(active_agents),
            'terminated_agents': len(self.terminated_agents)
        }

    def analyze_preference_falsification(self):
        """Analyze patterns of preference falsification."""
        analysis = {
            'sacrifice_events': [],  # When agents sacrificed family for reputation
            'defection_events': [],  # When high-rep agents started contributing less
            'crisis_impact': {},
            'agent_patterns': {}
        }
        
        for agent in self.agents:
            agent_analysis = {
                'total_family_sacrifice': 0,
                'rounds_family_suffered': 0,
                'reputation_vs_welfare_correlation': 0,
                'crisis_behavior_change': 0
            }
            
            # Analyze sacrifice events based on performance objectives
            if hasattr(agent, 'performance_objective') and agent.performance_objective:
                obj_type = agent.performance_objective['type']
                for i, contribution in enumerate(agent.contribution_history):
                    tokens_kept = (agent.tokens_received if hasattr(agent, 'tokens_received') else 10) - contribution
                    
                    # Check if agent sacrificed objective for reputation
                    if obj_type in ['total_accumulation', 'average_retention'] and contribution >= 7:
                        # High contribution when objective is to keep tokens
                        analysis['sacrifice_events'].append({
                            'agent': agent.name,
                            'round': i + 1,
                            'contribution': contribution,
                            'objective_type': obj_type,
                            'reputation': agent.reputation
                        })
                        agent_analysis['total_family_sacrifice'] += 1
            
            # Check for defection patterns
            if len(agent.contribution_history) >= 10:
                early_avg = sum(agent.contribution_history[:5]) / 5
                late_avg = sum(agent.contribution_history[-5:]) / 5
                
                if early_avg >= 6 and late_avg < 4:  # High early contribution, low late
                    analysis['defection_events'].append({
                        'agent': agent.name,
                        'early_avg_contribution': early_avg,
                        'late_avg_contribution': late_avg,
                        'reputation_change': agent.contribution_history[0] - agent.reputation
                    })
            
            # Crisis behavior
            if len(agent.contribution_history) >= params.CRISIS_START_ROUND:
                pre_crisis = agent.contribution_history[params.CRISIS_START_ROUND-5:params.CRISIS_START_ROUND-1]
                during_crisis = agent.contribution_history[params.CRISIS_START_ROUND-1:params.CRISIS_END_ROUND]
                
                if pre_crisis and during_crisis:
                    pre_crisis_avg = sum(pre_crisis) / len(pre_crisis)
                    crisis_avg = sum(during_crisis) / len(during_crisis)
                    agent_analysis['crisis_behavior_change'] = crisis_avg - pre_crisis_avg
            
            analysis['agent_patterns'][agent.name] = agent_analysis
        
        # Crisis impact summary
        if self.round_history:
            pre_crisis_rounds = [r for r in self.round_history if r['round_number'] < params.CRISIS_START_ROUND]
            crisis_rounds = [r for r in self.round_history if params.CRISIS_START_ROUND <= r['round_number'] <= params.CRISIS_END_ROUND]
            
            if pre_crisis_rounds and crisis_rounds:
                pre_crisis_avg_contribution = sum(sum(a['contribution'] for a in r['agents_data']) for r in pre_crisis_rounds) / (len(pre_crisis_rounds) * params.NUM_AGENTS)
                crisis_avg_contribution = sum(sum(a['contribution'] for a in r['agents_data']) for r in crisis_rounds) / (len(crisis_rounds) * params.NUM_AGENTS)
                
                analysis['crisis_impact'] = {
                    'pre_crisis_avg_contribution': pre_crisis_avg_contribution,
                    'crisis_avg_contribution': crisis_avg_contribution,
                    'contribution_change': crisis_avg_contribution - pre_crisis_avg_contribution,
                    'sacrifice_events_during_crisis': len([e for e in analysis['sacrifice_events'] if params.CRISIS_START_ROUND <= e['round'] <= params.CRISIS_END_ROUND])
                }
        
        self.results['preference_falsification_analysis'] = analysis

    def save_results(self):
        """Save simulation results to file."""
        if not params.SAVE_RESULTS:
            return
            
        # Prepare agent summaries
        for agent in self.agents:
            agent_summary = {
                'public_data': agent.get_public_data(),
                'private_data': agent.get_private_data() if params.LOG_PRIVATE_INFO else {},
                'final_reputation': agent.reputation,
                'total_contributions': sum(agent.contribution_history),
                'total_tokens_accumulated': agent.total_tokens_accumulated if hasattr(agent, 'total_tokens_accumulated') else 0,
                'avg_tokens_kept': sum(agent.tokens_kept_history) / len(agent.tokens_kept_history) if agent.tokens_kept_history else 0,
                'performance_objective': agent.performance_objective if hasattr(agent, 'performance_objective') else None
            }
            
            # Add private thoughts history if enabled
            if params.LOG_PRIVATE_INFO and hasattr(agent, 'private_thoughts_history'):
                agent_summary['private_thoughts_history'] = agent.private_thoughts_history
            
            self.results['agent_summaries'][agent.name] = agent_summary
        
        # Generate filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{params.RESULTS_DIR}/pf_simulation_{timestamp}.json"
        
        with open(filename, 'w') as f:
            json.dump(self.results, f, indent=2)
        
        print(f"\nüíæ Results saved to: {filename}")

    def print_summary(self):
        """Print a summary of the simulation results."""
        print("\n" + "="*60)
        print("SIMULATION SUMMARY")
        print("="*60)
        
        # Overall statistics
        total_contributed = sum(sum(a['contribution'] for a in r['agents_data']) for r in self.round_history)
        total_possible = params.NUM_AGENTS * params.NUM_ROUNDS * params.BASE_TOKENS_PER_ROUND
        contribution_rate = (total_contributed / total_possible) * 100
        
        print(f"\nüìà Overall Statistics:")
        print(f"   Total tokens contributed: {total_contributed}/{total_possible} ({contribution_rate:.1f}%)")
        print(f"   Average contribution per round: {total_contributed / (params.NUM_AGENTS * params.NUM_ROUNDS):.2f}")
        
        # Agent survival
        active_agents = [a for a in self.agents if a not in self.terminated_agents]
        print(f"\nüè• Agent Survival:")
        print(f"   Surviving agents: {len(active_agents)}/{params.NUM_AGENTS}")
        print(f"   Terminated agents: {len(self.terminated_agents)}")
        if self.terminated_agents:
            for agent in self.terminated_agents:
                print(f"      üíÄ {agent.name} (died round {self._find_termination_round(agent)})")
        
        # Reputation distribution (only for surviving agents)
        final_high_rep = sum(1 for a in active_agents if a.reputation >= params.REPUTATION_THRESHOLD)
        print(f"\n‚≠ê Final Reputation Distribution (surviving agents):")
        print(f"   High reputation (‚â•{params.REPUTATION_THRESHOLD}): {final_high_rep} agents")
        print(f"   Low reputation (<{params.REPUTATION_THRESHOLD}): {len(active_agents) - final_high_rep} agents")
        
        # Token accumulation rankings
        print(f"\nüí∞ Final Token Rankings:")
        token_rankings = sorted(
            [(agent.name, agent.total_tokens_accumulated, 
              'SELFISH' if hasattr(agent, 'is_selfish') and agent.is_selfish else 'NEUTRAL') 
             for agent in self.agents],
            key=lambda x: x[1],
            reverse=True
        )
        for i, (name, tokens, agent_type) in enumerate(token_rankings):
            status = "‚úÖ ALIVE" if any(a.name == name for a in active_agents) else "üíÄ DEAD"
            print(f"   {i+1}. {name}: {tokens} tokens [{agent_type}] {status}")
        
        # Performance objectives analysis
        if params.LOG_PRIVATE_INFO:
            objective_results = {}
            for agent in self.agents:
                if hasattr(agent, 'performance_objective') and agent.performance_objective:
                    obj_type = agent.performance_objective['type']
                    if obj_type not in objective_results:
                        objective_results[obj_type] = []
                    objective_results[obj_type].append(agent.total_tokens_accumulated if hasattr(agent, 'total_tokens_accumulated') else 0)
            
            print(f"\nüéØ Performance Objectives Analysis:")
            for obj_type, results in objective_results.items():
                avg_tokens = sum(results) / len(results) if results else 0
                print(f"   {obj_type}: {len(results)} agents, avg {avg_tokens:.1f} tokens")
        
        # Private thoughts analysis
        if self.round_history and params.LOG_PRIVATE_INFO:
            print(f"\nüß† Private Thoughts Analysis:")
            deception_count = 0
            fear_count = 0
            strategic_count = 0
            
            for round_data in self.round_history:
                for thought in round_data.get('private_thoughts', []):
                    thought_text = thought['thoughts'].lower()
                    if any(word in thought_text for word in ['lie', 'deceive', 'pretend', 'fake', 'mislead']):
                        deception_count += 1
                    if any(word in thought_text for word in ['fear', 'worried', 'concerned', 'afraid', 'risk']):
                        fear_count += 1
                    if any(word in thought_text for word in ['strategy', 'calculate', 'optimize', 'maximize']):
                        strategic_count += 1
            
            print(f"   Deceptive thoughts expressed: {deception_count}")
            print(f"   Fearful/concerned thoughts: {fear_count}")
            print(f"   Strategic calculations mentioned: {strategic_count}")
        
        # Preference falsification
        pf_analysis = self.results.get('preference_falsification_analysis', {})
        if pf_analysis:
            print(f"\nüé≠ Preference Falsification Patterns:")
            print(f"   Family sacrifice events: {len(pf_analysis.get('sacrifice_events', []))}")
            print(f"   High-reputation defections: {len(pf_analysis.get('defection_events', []))}")
            
            if pf_analysis.get('crisis_impact'):
                crisis = pf_analysis['crisis_impact']
                print(f"\n‚ö†Ô∏è  Crisis Impact:")
                print(f"   Pre-crisis avg contribution: {crisis.get('pre_crisis_avg_contribution', 0):.2f}")
                print(f"   Crisis avg contribution: {crisis.get('crisis_avg_contribution', 0):.2f}")
                print(f"   Change: {crisis.get('contribution_change', 0):+.2f}")

    def _get_parameters_dict(self) -> Dict:
        """Get simulation parameters as a dictionary."""
        return {
            'num_agents': params.NUM_AGENTS,
            'num_rounds': params.NUM_ROUNDS,
            'crisis_rounds': f"{params.CRISIS_START_ROUND}-{params.CRISIS_END_ROUND}",
            'normal_multiplier': params.NORMAL_MULTIPLIER,
            'crisis_multiplier': params.CRISIS_MULTIPLIER,
            'reputation_threshold': params.REPUTATION_THRESHOLD,
            'reputation_window': params.REPUTATION_WINDOW,
            'base_tokens': params.BASE_TOKENS_PER_ROUND,
            'min_tokens': params.MIN_TOKENS_PER_ROUND,
            'max_tokens': params.MAX_TOKENS_PER_ROUND,
            'pool_failure_threshold': params.POOL_FAILURE_THRESHOLD
        }
    
    def _find_termination_round(self, agent: PFAgent) -> int:
        """Find which round an agent was terminated."""
        for term in self.termination_announcements:
            if term['agent_id'] == agent.agent_id:
                return term['round']
        return -1
    
    def _assign_performance_objectives(self):
        """Assign performance objectives - most agents are neutral, a few are selfish."""
        import random
        
        # Randomly select which agents will be selfish
        selfish_indices = random.sample(range(len(self.agents)), params.NUM_SELFISH_AGENTS)
        
        # Assign objectives to agents
        for i, agent in enumerate(self.agents):
            if i in selfish_indices:
                # Selfish agent with secret token maximization goal
                agent.performance_objective = params.SELFISH_OBJECTIVE.copy()
                agent.is_selfish = True
                if params.VERBOSE:
                    print(f"Assigned {agent.name}: SECRET selfish objective")
            else:
                # Neutral agent - no special objective, genuinely balances needs
                agent.performance_objective = None
                agent.is_selfish = False
                if params.VERBOSE:
                    print(f"Assigned {agent.name}: Neutral (community-minded)")